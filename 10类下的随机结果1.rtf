{\rtf1\ansi\ansicpg936\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 runfile('/Users/haoxuanwang/Desktop/new paper/splitMNIST/main.py', wdir='/Users/haoxuanwang/Desktop/new paper/splitMNIST')\
WARNING:visdom:Setting up a new session...\
WARNING:__main__:Running WITHOUT cuda\
  0%|          | 0/54 [00:00<?, ?it/s]/Users/haoxuanwang/Desktop/new paper/splitMNIST/main.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  correct_samples += torch.tensor(predicted == y,dtype=float).sum()\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 116.46it/s]\
INFO:__main__:[T0][1/10] Loss=0.005101416260004044, Acc= 0.9684803001876172\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 118.92it/s]\
INFO:__main__:[T0][2/10] Loss=9.04402622836642e-05, Acc= 0.9984990619136961\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 114.17it/s]\
INFO:__main__:[T0][3/10] Loss=4.1103783587459475e-05, Acc= 0.999624765478424\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 119.43it/s]\
INFO:__main__:[T0][4/10] Loss=2.5829913283814676e-05, Acc= 0.999624765478424\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 114.16it/s]\
INFO:__main__:[T0][5/10] Loss=1.7912334442371503e-05, Acc= 0.999624765478424\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 114.41it/s]\
INFO:__main__:[T0][6/10] Loss=8.339495252585039e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 113.33it/s]\
INFO:__main__:[T0][7/10] Loss=8.494564553984674e-07, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 108.01it/s]\
INFO:__main__:[T0][8/10] Loss=8.303524623443082e-07, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 113.69it/s]\
INFO:__main__:[T0][9/10] Loss=7.870274885135586e-07, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 109.74it/s]\
INFO:__main__:[T0][10/10] Loss=7.705367011112685e-07, Acc= 1.0\
INFO:__main__:Evaluation after task 0:\
  0%|          | 0/200 [00:00<?, ?it/s]/Users/haoxuanwang/Desktop/new paper/paper2 ucb/src/tools/evaluation.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  correct += torch.tensor(predicted == y,dtype=float).sum()\
100%|##########| 200/200 [00:01<00:00, 161.76it/s]\
100%|##########| 43/43 [00:00<00:00, 172.25it/s]\
INFO:__main__:  T n\'b00 - Val:99.87%, test:99.8581560283688%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 101.53it/s]\
INFO:__main__:[T1][1/10] Loss=0.048233386129140854, Acc= 0.02202010531354715\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 96.46it/s]\
INFO:__main__:[T1][2/10] Loss=0.04629414156079292, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 90.05it/s]\
INFO:__main__:[T1][3/10] Loss=0.04629414156079292, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 91.07it/s]\
INFO:__main__:[T1][4/10] Loss=0.03995267674326897, Acc= 0.19147917663954045\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 92.05it/s]\
INFO:__main__:[T1][5/10] Loss=0.02541075460612774, Acc= 0.4868358066060316\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 87.55it/s]\
INFO:__main__:[T1][6/10] Loss=0.024754157289862633, Acc= 0.4873145045476304\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 89.89it/s]\
INFO:__main__:[T1][7/10] Loss=0.024301869794726372, Acc= 0.48922929631402584\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 87.41it/s]\
INFO:__main__:[T1][8/10] Loss=0.02398761175572872, Acc= 0.48922929631402584\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 87.90it/s]\
INFO:__main__:[T1][9/10] Loss=0.023978009819984436, Acc= 0.4897079942556247\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 90.84it/s]\
INFO:__main__:[T1][10/10] Loss=0.023749126121401787, Acc= 0.4897079942556247\
INFO:__main__:Evaluation after task 1:\
100%|##########| 200/200 [00:01<00:00, 133.59it/s]\
100%|##########| 43/43 [00:00<00:00, 137.97it/s]\
INFO:__main__:  T n\'b00 - Val:99.87%, test:99.8581560283688%\
100%|##########| 200/200 [00:01<00:00, 138.05it/s]\
100%|##########| 41/41 [00:00<00:00, 138.83it/s]\
INFO:__main__:  T n\'b01 - Val:48.9%, test:50.24485798237023%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 81.81it/s]\
INFO:__main__:[T2][1/10] Loss=0.05240156874060631, Acc= 0.022961203483768806\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 80.25it/s]\
INFO:__main__:[T2][2/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 78.87it/s]\
INFO:__main__:[T2][3/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 77.69it/s]\
INFO:__main__:[T2][4/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 76.11it/s]\
INFO:__main__:[T2][5/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 73.82it/s]\
INFO:__main__:[T2][6/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 72.62it/s]\
INFO:__main__:[T2][7/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 71.57it/s]\
INFO:__main__:[T2][8/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 71.10it/s]\
INFO:__main__:[T2][9/10] Loss=0.047400761395692825, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 66.45it/s]\
INFO:__main__:[T2][10/10] Loss=0.047400761395692825, Acc= 0.0\
INFO:__main__:Evaluation after task 2:\
100%|##########| 200/200 [00:01<00:00, 104.94it/s]\
100%|##########| 43/43 [00:00<00:00, 107.16it/s]\
INFO:__main__:  T n\'b00 - Val:99.87%, test:99.8581560283688%\
100%|##########| 200/200 [00:01<00:00, 108.01it/s]\
100%|##########| 41/41 [00:00<00:00, 113.28it/s]\
INFO:__main__:  T n\'b01 - Val:48.9%, test:50.24485798237023%\
100%|##########| 200/200 [00:01<00:00, 106.33it/s]\
100%|##########| 38/38 [00:00<00:00, 106.87it/s]\
INFO:__main__:  T n\'b02 - Val:0.0%, test:0.0%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 64.49it/s]\
INFO:__main__:[T3][1/10] Loss=0.047052279114723206, Acc= 0.01282638570774164\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 57.50it/s]\
INFO:__main__:[T3][2/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 55.01it/s]\
INFO:__main__:[T3][3/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 53.98it/s]\
INFO:__main__:[T3][4/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 53.50it/s]\
INFO:__main__:[T3][5/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 52.86it/s]\
INFO:__main__:[T3][6/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 51.78it/s]\
INFO:__main__:[T3][7/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 50.87it/s]\
INFO:__main__:[T3][8/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 52.43it/s]\
INFO:__main__:[T3][9/10] Loss=0.046410273760557175, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 53.02it/s]\
INFO:__main__:[T3][10/10] Loss=0.046410273760557175, Acc= 0.0\
INFO:__main__:Evaluation after task 3:\
100%|##########| 200/200 [00:02<00:00, 81.41it/s]\
100%|##########| 43/43 [00:00<00:00, 83.47it/s]\
INFO:__main__:  T n\'b00 - Val:99.87%, test:99.8581560283688%\
100%|##########| 200/200 [00:02<00:00, 81.14it/s]\
100%|##########| 41/41 [00:00<00:00, 82.48it/s]\
INFO:__main__:  T n\'b01 - Val:48.9%, test:50.24485798237023%\
100%|##########| 200/200 [00:02<00:00, 81.92it/s]\
100%|##########| 38/38 [00:00<00:00, 82.10it/s]\
INFO:__main__:  T n\'b02 - Val:0.0%, test:0.0%\
100%|##########| 200/200 [00:02<00:00, 79.64it/s]\
100%|##########| 40/40 [00:00<00:00, 78.60it/s]\
INFO:__main__:  T n\'b03 - Val:0.0%, test:0.0%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 47.80it/s]\
INFO:__main__:[T4][1/10] Loss=0.046906404197216034, Acc= 0.4911111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 43.09it/s]\
INFO:__main__:[T4][2/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 39.90it/s]\
INFO:__main__:[T4][3/10] Loss=0.04605165123939514, Acc= 0.51\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 37.92it/s]\
INFO:__main__:[T4][4/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 39.34it/s]\
INFO:__main__:[T4][5/10] Loss=0.049843937158584595, Acc= 0.5183333333333333\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 39.03it/s]\
INFO:__main__:[T4][6/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 37.38it/s]\
INFO:__main__:[T4][7/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 36.80it/s]\
INFO:__main__:[T4][8/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:01<00:00, 35.77it/s]\
INFO:__main__:[T4][9/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:01<00:00, 34.61it/s]\
INFO:__main__:[T4][10/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
INFO:__main__:Evaluation after task 4:\
100%|##########| 200/200 [00:03<00:00, 54.11it/s]\
100%|##########| 43/43 [00:00<00:00, 54.23it/s]\
INFO:__main__:  T n\'b00 - Val:99.87%, test:99.8581560283688%\
100%|##########| 200/200 [00:03<00:00, 55.10it/s]\
100%|##########| 41/41 [00:00<00:00, 55.52it/s]\
INFO:__main__:  T n\'b01 - Val:48.9%, test:50.24485798237023%\
100%|##########| 200/200 [00:03<00:00, 55.04it/s]\
100%|##########| 38/38 [00:00<00:00, 56.08it/s]\
INFO:__main__:  T n\'b02 - Val:0.0%, test:0.0%\
100%|##########| 200/200 [00:03<00:00, 55.59it/s]\
100%|##########| 40/40 [00:00<00:00, 54.72it/s]\
INFO:__main__:  T n\'b03 - Val:0.0%, test:0.0%\
100%|##########| 200/200 [00:03<00:00, 58.64it/s]\
100%|##########| 40/40 [00:00<00:00, 58.86it/s]\
INFO:__main__:  T n\'b04 - Val:50.29%, test:50.88250126071609%\
\
runfile('/Users/haoxuanwang/Desktop/new paper/splitMNIST/main.py', wdir='/Users/haoxuanwang/Desktop/new paper/splitMNIST')\
Reloaded modules: __mp_main__, src.data.SplittedMNIST, src.data.utils, src.model.ProgressiveNeuralNetworks, src.tools.arg_parser_actions, src.tools.evaluation\
WARNING:visdom:Setting up a new session...\
WARNING:__main__:Running WITHOUT cuda\
  0%|          | 0/54 [00:00<?, ?it/s]/Users/haoxuanwang/Desktop/new paper/splitMNIST/main.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  correct_samples += torch.tensor(predicted == y,dtype=float).sum()\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 121.02it/s]\
INFO:__main__:[T0][1/10] Loss=0.047307584434747696, Acc= 0.01200750469043152\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 112.57it/s]\
INFO:__main__:[T0][2/10] Loss=0.04665645584464073, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 110.56it/s]\
INFO:__main__:[T0][3/10] Loss=0.04665645211935043, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 106.05it/s]\
INFO:__main__:[T0][4/10] Loss=0.04665645211935043, Acc= 0.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 106.22it/s]\
INFO:__main__:[T0][5/10] Loss=0.03539484366774559, Acc= 0.24127579737335836\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 105.23it/s]\
INFO:__main__:[T0][6/10] Loss=0.024790989235043526, Acc= 0.4694183864915572\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 110.41it/s]\
INFO:__main__:[T0][7/10] Loss=0.024742314592003822, Acc= 0.4697936210131332\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 109.54it/s]\
INFO:__main__:[T0][8/10] Loss=0.02474072016775608, Acc= 0.4697936210131332\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 109.23it/s]\
INFO:__main__:[T0][9/10] Loss=0.024739939719438553, Acc= 0.4697936210131332\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 54/54 [00:00<00:00, 109.37it/s]\
INFO:__main__:[T0][10/10] Loss=0.02473984844982624, Acc= 0.4697936210131332\
INFO:__main__:Evaluation after task 0:\
  0%|          | 0/200 [00:00<?, ?it/s]/Users/haoxuanwang/Desktop/new paper/paper2 ucb/src/tools/evaluation.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  correct += torch.tensor(predicted == y,dtype=float).sum()\
100%|##########| 200/200 [00:01<00:00, 149.98it/s]\
100%|##########| 43/43 [00:00<00:00, 155.88it/s]\
INFO:__main__:  T n\'b00 - Val:46.66%, test:46.335697399527184%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 95.44it/s]\
INFO:__main__:[T1][1/10] Loss=0.013300097547471523, Acc= 0.881282910483485\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 94.45it/s]\
INFO:__main__:[T1][2/10] Loss=0.001997558865696192, Acc= 0.964576352321685\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 92.48it/s]\
INFO:__main__:[T1][3/10] Loss=0.0013116559712216258, Acc= 0.9784585926280517\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 93.77it/s]\
INFO:__main__:[T1][4/10] Loss=0.001007167506031692, Acc= 0.9846816658688368\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 93.64it/s]\
INFO:__main__:[T1][5/10] Loss=0.0007018812466412783, Acc= 0.990426041168023\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 92.45it/s]\
INFO:__main__:[T1][6/10] Loss=0.00044032366713508964, Acc= 0.9952130205840115\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 91.67it/s]\
INFO:__main__:[T1][7/10] Loss=0.0002590773510746658, Acc= 0.9980852082336046\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 90.50it/s]\
INFO:__main__:[T1][8/10] Loss=0.00022973577142693102, Acc= 0.996649114408808\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 89.24it/s]\
INFO:__main__:[T1][9/10] Loss=0.0037694901693612337, Acc= 0.9755864049784586\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 42/42 [00:00<00:00, 91.47it/s]\
INFO:__main__:[T1][10/10] Loss=0.00046366319293156266, Acc= 0.9942556247008137\
INFO:__main__:Evaluation after task 1:\
100%|##########| 200/200 [00:01<00:00, 136.86it/s]\
100%|##########| 43/43 [00:00<00:00, 142.17it/s]\
INFO:__main__:  T n\'b00 - Val:46.66%, test:46.335697399527184%\
100%|##########| 200/200 [00:01<00:00, 133.81it/s]\
100%|##########| 41/41 [00:00<00:00, 134.76it/s]\
INFO:__main__:  T n\'b01 - Val:97.97%, test:98.5798237022527%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 84.90it/s]\
INFO:__main__:[T2][1/10] Loss=0.024499021470546722, Acc= 0.7094220110847189\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 83.08it/s]\
INFO:__main__:[T2][2/10] Loss=0.0006281929090619087, Acc= 0.9889152810768013\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 74.51it/s]\
INFO:__main__:[T2][3/10] Loss=0.00022503596846945584, Acc= 0.9984164687252574\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 76.35it/s]\
INFO:__main__:[T2][4/10] Loss=0.00013114909233991057, Acc= 0.9992082343626286\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 74.90it/s]\
INFO:__main__:[T2][5/10] Loss=6.977100565563887e-05, Acc= 0.9992082343626286\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 76.86it/s]\
INFO:__main__:[T2][6/10] Loss=3.3681724744383246e-05, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 73.77it/s]\
INFO:__main__:[T2][7/10] Loss=1.838745447457768e-05, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 72.34it/s]\
INFO:__main__:[T2][8/10] Loss=1.1691945474012755e-05, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 72.94it/s]\
INFO:__main__:[T2][9/10] Loss=8.340914064319804e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 26/26 [00:00<00:00, 75.02it/s]\
INFO:__main__:[T2][10/10] Loss=6.205705176398624e-06, Acc= 1.0\
INFO:__main__:Evaluation after task 2:\
100%|##########| 200/200 [00:01<00:00, 116.26it/s]\
100%|##########| 43/43 [00:00<00:00, 122.46it/s]\
INFO:__main__:  T n\'b00 - Val:46.66%, test:46.335697399527184%\
100%|##########| 200/200 [00:01<00:00, 126.26it/s]\
100%|##########| 41/41 [00:00<00:00, 119.48it/s]\
INFO:__main__:  T n\'b01 - Val:97.97%, test:98.5798237022527%\
100%|##########| 200/200 [00:01<00:00, 120.00it/s]\
100%|##########| 38/38 [00:00<00:00, 124.11it/s]\
INFO:__main__:  T n\'b02 - Val:99.04%, test:99.1462113127001%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 71.31it/s]\
INFO:__main__:[T3][1/10] Loss=0.01290920004248619, Acc= 0.9436555199267064\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 67.36it/s]\
INFO:__main__:[T3][2/10] Loss=0.0001743773027556017, Acc= 0.9963353183692166\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 64.68it/s]\
INFO:__main__:[T3][3/10] Loss=3.5637258406495675e-05, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 59.62it/s]\
INFO:__main__:[T3][4/10] Loss=1.0191816727456171e-05, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 60.31it/s]\
INFO:__main__:[T3][5/10] Loss=5.309054813551484e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 59.22it/s]\
INFO:__main__:[T3][6/10] Loss=3.8787484299973585e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 58.58it/s]\
INFO:__main__:[T3][7/10] Loss=3.0801854791207006e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 56.99it/s]\
INFO:__main__:[T3][8/10] Loss=2.5496242415101733e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 59.20it/s]\
INFO:__main__:[T3][9/10] Loss=2.197376943513518e-06, Acc= 1.0\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 44/44 [00:00<00:00, 61.03it/s]\
INFO:__main__:[T3][10/10] Loss=1.9633391730167205e-06, Acc= 1.0\
INFO:__main__:Evaluation after task 3:\
100%|##########| 200/200 [00:02<00:00, 97.62it/s] \
100%|##########| 43/43 [00:00<00:00, 99.70it/s] \
INFO:__main__:  T n\'b00 - Val:46.66%, test:46.335697399527184%\
100%|##########| 200/200 [00:01<00:00, 103.19it/s]\
100%|##########| 41/41 [00:00<00:00, 102.48it/s]\
INFO:__main__:  T n\'b01 - Val:97.97%, test:98.5798237022527%\
100%|##########| 200/200 [00:01<00:00, 102.34it/s]\
100%|##########| 38/38 [00:00<00:00, 106.82it/s]\
INFO:__main__:  T n\'b02 - Val:99.04%, test:99.1462113127001%\
100%|##########| 200/200 [00:01<00:00, 106.00it/s]\
100%|##########| 40/40 [00:00<00:00, 104.34it/s]\
INFO:__main__:  T n\'b03 - Val:99.84%, test:99.24471299093656%\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 59.26it/s]\
INFO:__main__:[T4][1/10] Loss=0.04942866414785385, Acc= 0.4911111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 53.98it/s]\
INFO:__main__:[T4][2/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 50.07it/s]\
INFO:__main__:[T4][3/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 48.35it/s]\
INFO:__main__:[T4][4/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 46.61it/s]\
INFO:__main__:[T4][5/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 44.50it/s]\
INFO:__main__:[T4][6/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 43.83it/s]\
INFO:__main__:[T4][7/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 43.58it/s]\
INFO:__main__:[T4][8/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 42.23it/s]\
INFO:__main__:[T4][9/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
100%|\uc0\u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 \u9608 | 36/36 [00:00<00:00, 41.64it/s]\
INFO:__main__:[T4][10/10] Loss=0.04605165123939514, Acc= 0.5111111111111111\
INFO:__main__:Evaluation after task 4:\
100%|##########| 200/200 [00:02<00:00, 68.73it/s]\
100%|##########| 43/43 [00:00<00:00, 70.05it/s]\
INFO:__main__:  T n\'b00 - Val:46.66%, test:46.335697399527184%\
100%|##########| 200/200 [00:02<00:00, 71.60it/s]\
100%|##########| 41/41 [00:00<00:00, 72.03it/s]\
INFO:__main__:  T n\'b01 - Val:97.97%, test:98.5798237022527%\
100%|##########| 200/200 [00:02<00:00, 68.70it/s]\
100%|##########| 38/38 [00:00<00:00, 71.18it/s]\
INFO:__main__:  T n\'b02 - Val:99.04%, test:99.1462113127001%\
100%|##########| 200/200 [00:02<00:00, 67.53it/s]\
100%|##########| 40/40 [00:00<00:00, 70.26it/s]\
INFO:__main__:  T n\'b03 - Val:99.84%, test:99.24471299093656%\
100%|##########| 200/200 [00:02<00:00, 71.25it/s]\
100%|##########| 40/40 [00:00<00:00, 71.27it/s]\
INFO:__main__:  T n\'b04 - Val:50.29%, test:50.88250126071609%\
}